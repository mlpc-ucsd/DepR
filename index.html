<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="description" content="We propose DepR, a depth-guided single-view scene reconstruction framework that integrates instance-level diffusion within a compositional paradigm."><meta name="keywords" content="DepR, Single-view Reconstruction, 3D Reconstruction"><meta name="viewport" content="width=device-width,initial-scale=1"><title>DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-PPKLHKPJE8"></script><script>window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-PPKLHKPJE8');</script><link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.4/css/bulma.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"><link rel="stylesheet" href="./static/css/index.ecf86bea952339506761.css"><link rel="icon" href="./static/images/mlpc.webp"><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="https://kit.fontawesome.com/f887fab13f.js" crossorigin="anonymous"></script><script src="./static/js/index.21b59bd40255d961c5a1.js"></script><script type="importmap">{
      "imports": {
      "three": "https://cdn.jsdelivr.net/npm/three@0.148.0/build/three.module.js",
      "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.148.0/examples/jsm/"
      }
  }</script><script type="module" src="./static/js/visualizer.52ebf5f8eb0c1a3b9477.js"></script></head><body><nav class="navbar" role="navigation" aria-label="main navigation"><div class="navbar-brand"><a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"><span aria-hidden="true"></span> <span aria-hidden="true"></span> <span aria-hidden="true"></span></a></div><div class="navbar-menu"><div class="navbar-start" style="flex-grow: 1; justify-content: center;"><a class="navbar-item" href="https://github.com/mlpc-ucsd" target="_blank"><span class="icon"><i class="fas fa-home"></i></span></a><div class="navbar-item has-dropdown is-hoverable"><a class="navbar-link">More Research</a><div class="navbar-dropdown"><a class="navbar-item" href="https://github.com/mlpc-ucsd/Uni-3D" target="_blank">Uni-3D </a><a class="navbar-item" href="https://mlpc-ucsd.github.io/BDM" target="_blank">BDM</a></div></div></div></div></nav><section class="hero"><div class="hero-body hero-body-less-padding"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width has-text-centered"><h1 class="title is-1 publication-title">DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion</h1><div class="column is-full-width is-size-4"><span class="publication-venue"><b>ICCV 2025</b></span></div><div class="is-size-5 publication-authors"><span class="author-block"><a href="https://clarivy.github.io/" target="_blank">Qingcheng Zhao</a><sup>*,1,&dagger;</sup>,</span> <span class="author-block"><a href="https://xzhang.dev/" target="_blank">Xiang Zhang</a><sup>*,<i class="fa-regular fa-envelope"></i>,2</sup>,</span> <span class="author-block"><a href="https://xxuhaiyang.github.io/" target="_blank">Haiyang Xu</a><sup>2</sup>, </span><span class="author-block"><a href="https://zeyuan-chen.com/" target="_blank">Zeyuan Chen</a><sup>2</sup>, </span><span class="author-block"><a href="http://www.stat.ucla.edu/~jxie/" target="_blank">Jianwen Xie</a><sup>3</sup>, </span><span class="author-block"><a href="https://scholar.google.com/citations?user=z0BX_4wAAAAJ&hl=en" target="_blank">Yuan Gao</a><sup>4</sup>, </span><span class="author-block"><a href="https://pages.ucsd.edu/~ztu/" target="_blank">Zhuowen Tu</a><sup>2</sup></span></div><div class="is-size-6 publication-authors"><span class="author-block"><sup>*</sup> <b>equal contribution</b> &nbsp;&nbsp; <sup><i class="fa-regular fa-envelope"></i></sup> <b>corresponding author</b></span></div><div class="is-size-6 publication-authors"><span class="author-block" style="font-size: 90%;"><sup>&dagger;</sup> <i>Project done while Qingcheng Zhao interned at UC San Diego.</i></span></div><div class="is-size-5 publication-authors"><span class="author-block institution"><sup>1</sup>ShanghaiTech University</span> <span class="author-block institution"><sup>2</sup>UC San Diego</span> <span class="author-block institution"><sup>3</sup>Lambda, Inc.</span> <span class="author-block institution"><sup>4</sup>Stanford University</span></div><div class="column has-text-centered"><div class="publication-links"><span class="link-block"><a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_DepR_Depth_Guided_Single-view_Scene_Reconstruction_with_Instance-level_Diffusion_ICCV_2025_paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank"><span class="icon"><i class="fas fa-file-pdf"></i> </span><span>Paper</span> </a></span><span class="link-block"><a href="https://arxiv.org/abs/2507.22825" class="external-link button is-normal is-rounded is-dark" target="_blank"><span class="icon"><i class="ai ai-arxiv"></i> </span><span>arXiv</span> </a></span><span class="link-block"><a href="https://github.com/mlpc-ucsd/DepR" class="external-link button is-normal is-rounded is-dark" target="_blank"><span class="icon"><i class="fa-brands fa-github"></i> </span><span>Code</span></a></span></div></div></div></div></div></div></section><section class="section section-less-padding"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-half has-text-centered"><div class="title is-4 abstract">Input Image</div><img class="img-fluid" id="scene-img" alt=""></div><div class="column is-half has-text-centered"><div class="title is-4 abstract">Reconstructed scene <span style="color:grey;">(interactive)</span></div><canvas id="canvas-scene" data-engine="three.js r148" width="1080" height="810" style="touch-action: none;"></canvas></div><div class="loading-overlay" style="display: flex;"><div class="loading-spinner"></div><div class="loading-progress"></div></div></div><div id="comp-buttons" class="buttons is-centered is-vcentered has-text-centered"></div><div id="scene-buttons" class="columns is-centered is-vcentered has-text-centered"><div style="padding: 1rem;"><p class="title is-4 abstract">Scenes</p></div><input type="image" src="static/scenes/0/rgb.webp" class="column img-button" id="btn-s0"> <input type="image" src="static/scenes/1/rgb.webp" class="column img-button" id="btn-s1"> <input type="image" src="static/scenes/2/rgb.webp" class="column img-button" id="btn-s2"> <input type="image" src="static/scenes/3/rgb.webp" class="column img-button" id="btn-s3"> <input type="image" src="static/scenes/4/rgb.webp" class="column img-button" id="btn-s4"> <input type="image" src="static/scenes/5/rgb.webp" class="column img-button" id="btn-s5"></div></div></section><section class="section section-less-padding"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Abstract</h2><div class="content has-text-justified"><p>We propose <span class="depr">DepR</span>, a depth-guided single-view scene reconstruction framework that integrates instance-level diffusion within a compositional paradigm. Instead of reconstructing the entire scene holistically, <span class="depr">DepR</span> generates individual objects and subsequently composes them into a coherent 3D layout.</p><p>Unlike previous methods that use depth solely for object layout estimation during inference and therefore fail to fully exploit its rich geometric information, <span class="depr">DepR</span> leverages depth throughout both training and inference. Specifically, we introduce depth-guided conditioning to effectively encode shape priors into diffusion models. During inference, depth further guides DDIM sampling and layout optimization, enhancing alignment between the reconstruction and the input image. Despite being trained on limited synthetic data, <span class="depr">DepR</span> achieves state-of-the-art performance and demonstrates strong generalization in singleview scene reconstruction, as shown through evaluations on both synthetic and real-world datasets.</p></div></div></div></div></section><section class="section section-less-padding"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Method</h2><div class="has-text-centered"><img src="./static/images/pipeline.webp" alt="DepR Pipeline"></div><div class="content has-text-justified"><p><b>Overview of our <span class="depr">DepR</span>.</b> Depth is utilized in three key stages: 1) to back-project features to condition the latent tri-plane diffusion model to generate complete 3D shapes; 2) to guide the diffusion sampling process via gradients from a depth loss; and 3) to optimize object poses via layout loss for accurate scene composition.</p></div></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Comparisons on 3D-FRONT</h2><div class="has-text-centered"><img src="./static/images/qualitative_synth.webp" alt="Qualitative comparisons on synthetic dataset 3D-FRONT"></div></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Comparisons on Pix-3D and Our Own Images</h2><div class="has-text-centered"><img src="./static/images/qualitative_real.webp" alt="Qualitative comparisons on Pix-3D dataset and our own images"></div></div></div></div></section><section class="section" id="BibTeX"><div class="container is-max-desktop content"><h2 class="title">BibTeX</h2><pre><code>
  @InProceedings{Zhao_2025_ICCV_DepR,
      author    = {Zhao, Qingcheng and Zhang, Xiang and Xu, Haiyang and Chen, Zeyuan and Xie, Jianwen and Gao, Yuan and Tu, Zhuowen},
      title     = {DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion},
      booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
      month     = {October},
      year      = {2025},
      pages     = {5722-5733}
  }
      </code></pre></div></section><footer class="footer"><div class="container"><div class="content has-text-centered"><a class="icon-link" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_DepR_Depth_Guided_Single-view_Scene_Reconstruction_with_Instance-level_Diffusion_ICCV_2025_paper.pdf" target="_blank"><i class="fas fa-file-pdf"></i> </a><a class="icon-link" href="https://github.com/mlpc-ucsd/DepR" target="_blank" class="external-link" disabled="disabled"><i class="fa-brands fa-github"></i></a></div><div class="columns is-centered"><div class="column is-8"><div class="content"><p>This website is adapted from the amazing template of <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">nerfies</a>. Thanks to <a href="https://andreeadogaru.github.io/Gen3DSR/" target="_blank">Gen3DSR </a>for the interactive visualizer.</p></div></div></div></div></footer></body></html>